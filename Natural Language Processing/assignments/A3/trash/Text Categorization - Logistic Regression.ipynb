{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of naive Bayes and logistic regression for text categorization\n",
    "\n",
    "Adapted from https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load subset of \"20 Newsgroups\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"misc.forsale\", \"sci.space\", \n",
    "              \"sci.electronics\", \"comp.graphics\"]\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories, \n",
    "                                  shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 categories=categories, \n",
    "                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in twenty_train.target[:5]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and vectorize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3, stop_words=\"english\").fit(twenty_train.data)\n",
    "X_train = vectorizer.transform(twenty_train.data)\n",
    "X_test = vectorizer.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nb_model = MultinomialNB(alpha=1.0).fit(X_train, y_train)\n",
    "y_hat_nb_test = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat_nb_test, \n",
    "                            target_names=twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr_model = LogisticRegression(penalty=\"none\", \n",
    "                              multi_class=\"multinomial\",\n",
    "                              solver=\"lbfgs\").fit(X_train, y_train)\n",
    "y_hat_lr_test = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat_lr_test, \n",
    "                            target_names=twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr2_model = LogisticRegression(penalty=\"l2\", \n",
    "                               solver=\"lbfgs\",\n",
    "                               multi_class=\"multinomial\",\n",
    "                               max_iter=1000,\n",
    "                               C=10).fit(X_train, y_train)\n",
    "y_hat_lr2_test = lr2_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat_lr2_test, \n",
    "                            target_names=twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of train/test performance across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\"Naive Bayes\": nb_model,\n",
    "              \"Logistic Regression\": lr_model,\n",
    "              \"L2 Regularized LR\": lr2_model}\n",
    "plot_data = []\n",
    "for name, model in model_info.items():\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    plot_data.append([name, \"Train\", train_acc])\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    plot_data.append([name, \"Test\", test_acc])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.ylim((0.9,1))\n",
    "plot_df = pd.DataFrame(plot_data, columns=[\"model\", \"dataset\", \"accuracy\"])\n",
    "sns.lineplot(data=plot_df, \n",
    "             sort=False,\n",
    "             x=\"dataset\", \n",
    "             y=\"accuracy\", \n",
    "             hue=\"model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "vocab = {idx: w for w, idx in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = {}\n",
    "for i, c in enumerate(twenty_train.target_names):\n",
    "    top_features = np.argsort(nb_model.feature_log_prob_[i,:])[-1:-11:-1]\n",
    "    logprobs = nb_model.feature_log_prob_[i,top_features]\n",
    "    words = [vocab[x] for x in top_features]\n",
    "    word_data[f\"{c}_P(w|c)\"] = [np.exp(x) for x in logprobs]\n",
    "    word_data[f\"{c}_words\"] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(word_data).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = {}\n",
    "for i, c in enumerate(twenty_train.target_names):\n",
    "    top_features = np.argsort(lr2_model.coef_[i,:])[-1:-11:-1]\n",
    "    coefs = lr2_model.coef_[i,top_features]\n",
    "    words = [vocab[x] for x in top_features]\n",
    "    word_data[f\"{c}_beta\"] = coefs\n",
    "    word_data[f\"{c}_words\"] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(word_data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('data.csv', encoding='ISO-8859-1');\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    sms = dataset.iloc[i, 1]\n",
    "\n",
    "    # remove non alphabatic characters\n",
    "    sms = re.sub('[^A-Za-z]', ' ', sms)\n",
    "\n",
    "    # make words lowercase, because Go and go will be considered as two words\n",
    "    sms = sms.lower()\n",
    "\n",
    "    # tokenising\n",
    "    tokenized_sms = wt(sms)\n",
    "\n",
    "    # remove stop words and stemming\n",
    " \n",
    "    sms_processed = []\n",
    "    for word in tokenized_sms:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            sms_processed.append(spell(stemmer.stem(word)))\n",
    "\n",
    "    sms_text = \" \".join(sms_processed)\n",
    "    data.append(sms_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the feature matrix \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(max_features=1000)\n",
    "dataset.iloc[:, 0]\n",
    "\n",
    "X = matrix.fit_transform(data).toarray()\n",
    "y = dataset.iloc[:, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import random as rn\n",
    "#All this for reproducibility\n",
    "np.random.seed(1)\n",
    "rn.seed(1)\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopw = stopwords.words('english')\n",
    "\n",
    "wvLength = 300\n",
    "vectorSource = str(sys.argv[1]) # none, fasttext, custom-fasttext\n",
    "\n",
    "def tokenize (text):        #   no punctuation & starts with a letter & between 2-15 characters in length\n",
    "    tokens = [word.strip(string.punctuation) for word in RegexpTokenizer(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b').tokenize(text)]\n",
    "    return  [f.lower() for f in tokens if f and f.lower() not in nltk_stopw]\n",
    "\n",
    "def get20News():\n",
    "    X, labels, labelToName = [], [], {}\n",
    "    twenty_news = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n",
    "    for i, article in enumerate(twenty_news['data']):\n",
    "        stopped = tokenize (article)\n",
    "        if (len(stopped) == 0):\n",
    "            continue\n",
    "        groupIndex = twenty_news['target'][i]\n",
    "        X.append(stopped)\n",
    "        labels.append(groupIndex)\n",
    "        labelToName[groupIndex] = twenty_news['target_names'][groupIndex]\n",
    "    nTokens = [len(x) for x in X]\n",
    "    return X, np.array(labels), labelToName, nTokens\n",
    "\n",
    "def getEmbeddingMatrix (word_index, vectorSource):\n",
    "    wordVecSources = {'fasttext' : './vectors/crawl-300d-2M-subword.vec', 'custom-fasttext' : './vectors/' + '20news-fasttext.json' }\n",
    "    f = open (wordVecSources[vectorSource])\n",
    "    allWv = {}\n",
    "    if (vectorSource == 'custom-fasttext'):\n",
    "        allWv = json.loads(f.read())\n",
    "    elif (vectorSource == 'fasttext'):\n",
    "        errorCount = 0\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0].strip()\n",
    "            try:\n",
    "                wv = np.asarray(values[1:], dtype='float32')\n",
    "                if (len(wv) != wvLength):\n",
    "                    errorCount = errorCount + 1\n",
    "                    continue\n",
    "            except:\n",
    "                errorCount = errorCount + 1\n",
    "                continue\n",
    "            allWv[word] = wv\n",
    "        print (\"# Bad Word Vectors:\", errorCount)\n",
    "    f.close()\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, wvLength))  # +1 for the masked 0\n",
    "    for word, i in word_index.items():\n",
    "        if word in allWv:\n",
    "            embedding_matrix[i] = allWv[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "def sparseMultiply (sparseX, corpus_embedding_matrix):\n",
    "    denseZ = []\n",
    "    for row in sparseX:\n",
    "        newRow = np.zeros(wvLength)\n",
    "        for nonzeroLocation, value in list(zip(row.indices, row.data)):\n",
    "            newRow = newRow + value * corpus_embedding_matrix[nonzeroLocation]\n",
    "        denseZ.append(newRow)\n",
    "    denseZ = np.array([np.array(xi) for xi in denseZ])\n",
    "    return denseZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going', 'put', 'end', 'non', 'pittsburghers', 'relief', 'bit', 'praise', 'pens', 'man', 'killing', 'devils', 'worse', 'thought', 'jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fun', 'watch', 'playoffs', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'pens', 'going', 'beat', 'pulp', 'jersey', 'anyway', 'disappointed', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game', 'pens', 'rule']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "70\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[(0, 'alt.atheism'), (1, 'comp.graphics'), (2, 'comp.os.ms-windows.misc'), (3, 'comp.sys.ibm.pc.hardware'), (4, 'comp.sys.mac.hardware'), (5, 'comp.windows.x'), (6, 'misc.forsale'), (7, 'rec.autos'), (8, 'rec.motorcycles'), (9, 'rec.sport.baseball'), (10, 'rec.sport.hockey'), (11, 'sci.crypt'), (12, 'sci.electronics'), (13, 'sci.med'), (14, 'sci.space'), (15, 'soc.religion.christian'), (16, 'talk.politics.guns'), (17, 'talk.politics.mideast'), (18, 'talk.politics.misc'), (19, 'talk.religion.misc')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, labels, labelToName, nTokens = get20News()\n",
    "print(X[0])\n",
    "\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "print(labels[0])\n",
    "\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "print(nTokens[0])\n",
    "\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "# print (np.amin(nTokens), np.mean(nTokens),np.median(nTokens),np.std(nTokens),np.percentile(nTokens,85),np.percentile(nTokens,86),np.percentile(nTokens,87),np.percentile(nTokens,88),np.percentile(nTokens,89),np.percentile(nTokens,90),np.percentile(nTokens,91),np.percentile(nTokens,92),np.percentile(nTokens,93),np.percentile(nTokens,94),np.percentile(nTokens,95),np.percentile(nTokens,99),np.amax(nTokens))\n",
    "\n",
    "labelToNameSortedByLabel = sorted(labelToName.items(), key=lambda kv: kv[0]) # List of tuples sorted by the label number [ (0, ''), (1, ''), .. ]\n",
    "print(labelToNameSortedByLabel)\n",
    "namesInLabelOrder = [item[1] for item in labelToNameSortedByLabel]\n",
    "numClasses = len(namesInLabelOrder)\n",
    "print ('X, labels #classes classes {} {} {} {}'.format(len(X), str(labels.shape), numClasses, namesInLabelOrder))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=np.array([np.array(xi) for xi in X])          #   rows: Docs. columns: words\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=1).fit(X)\n",
    "word_index = vectorizer.vocabulary_\n",
    "Xencoded = vectorizer.transform(X)\n",
    "print ('Vocab sparse-Xencoded {} {}'.format(len(word_index), str(Xencoded.shape)))\n",
    "\n",
    "if (vectorSource != 'none'):\n",
    "    embedding_matrix = getEmbeddingMatrix (word_index, vectorSource)\n",
    "    Xencoded = sparseMultiply (Xencoded, embedding_matrix)\n",
    "    print ('Dense-Xencoded {}'.format(str(Xencoded.shape)))\n",
    "\n",
    "# Test & Train Split\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1).split(Xencoded, labels)\n",
    "train_indices, test_indices = next(sss)\n",
    "train_x, test_x = Xencoded[train_indices], Xencoded[test_indices]\n",
    "train_labels, test_labels = labels[train_indices], labels[test_indices]\n",
    "start_time = time.time()\n",
    "model = LinearSVC(tol=1.0e-6,max_iter=20000,verbose=1)\n",
    "model.fit(train_x, train_labels)\n",
    "predicted_labels = model.predict(test_x)\n",
    "elapsed_time = time.time() - start_time\n",
    "results = {}\n",
    "results['confusion_matrix'] = confusion_matrix(test_labels, predicted_labels).tolist()\n",
    "results['classification_report'] = classification_report(test_labels, predicted_labels, digits=4, target_names=namesInLabelOrder, output_dict=True)\n",
    "\n",
    "print (confusion_matrix(labels[test_indices], predicted_labels))\n",
    "print (classification_report(labels[test_indices], predicted_labels, digits=4, target_names=namesInLabelOrder))\n",
    "print ('Time Taken:', elapsed_time)\n",
    "results['elapsed_time'] = elapsed_time        # seconds\n",
    "\n",
    "f = open ('svm-' + vectorSource + '.json','w')\n",
    "out = json.dumps(results, ensure_ascii=True)\n",
    "f.write(out)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
