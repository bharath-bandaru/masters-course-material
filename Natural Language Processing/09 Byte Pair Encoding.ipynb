{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of byte-pair encoding\n",
    "\n",
    "Code adapted from https://leimao.github.io/blog/Byte-Pair-Encoding/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(word_gen):\n",
    "    \"\"\"Initialize vocabulary from corpus\n",
    "    \"\"\"\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for word in word_gen:\n",
    "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Calculate co-occurrence statistics for token bigrams\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"Merge two tokens and update the resulting vocabulary\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_tokens_from_vocab(vocab):\n",
    "    \"\"\"Recover tokens from tokenized vocabulary\n",
    "    \"\"\"\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/BharathBandaru/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['t', 'h', 'e', '</w>', 'f', 'u', 'l', 'o', 'n', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'p', 'm', 'k', 'x', 'w', 'b', 'z', 'q'])\n",
      "Number of tokens: 27\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "\n",
    "vocab = get_vocab(map(lambda x: x.lower(),\n",
    "                      filter(lambda x: x.isalpha(), \n",
    "                             brown.words())))\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_bpe(v):\n",
    "    pairs = get_stats(v)\n",
    "    if not pairs:\n",
    "        return v\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    v = merge_vocab(best, v)\n",
    "    return v, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('e', '</w>')\n",
      "All tokens: dict_keys(['t', 'h', 'e</w>', 'f', 'u', 'l', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'e', 'p', 'm', 'k', 'x', 'w', 'b', 'z', 'q'])\n",
      "Number of tokens: 28\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('t', 'h')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'e', 'p', 'm', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 29\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('s', '</w>')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'e', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 30\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: ('d', '</w>')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'v', 'e', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 31\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: ('t', '</w>')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 32\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('i', 'n')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 33\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('e', 'r')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 34\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('a', 'n')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'an', 'd</w>', 'j', 's', 'a', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 35\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('th', 'e</w>')\n",
      "All tokens: dict_keys(['the</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'an', 'd</w>', 'j', 's', 'a', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 'e</w>', 'th', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 36\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('o', 'n')\n",
      "All tokens: dict_keys(['the</w>', 'f', 'u', 'l', 't', 'on', '</w>', 'c', 'o', 'n', 'y', 'g', 'r', 'an', 'd</w>', 'j', 's', 'a', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 'e</w>', 'th', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 37\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    print('Iter: {}'.format(i))\n",
    "    vocab, best = step_bpe(vocab)\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('Best pair: {}'.format(best))\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the</w>',\n",
       " 'f',\n",
       " 'u',\n",
       " 'l',\n",
       " 't',\n",
       " 'on',\n",
       " '</w>',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'y',\n",
       " 'g',\n",
       " 'r',\n",
       " 'an',\n",
       " 'd</w>',\n",
       " 'j',\n",
       " 's',\n",
       " 'a',\n",
       " 'i',\n",
       " 'd',\n",
       " 'in',\n",
       " 'v',\n",
       " 'e',\n",
       " 't</w>',\n",
       " 'p',\n",
       " 'm',\n",
       " 'e</w>',\n",
       " 'th',\n",
       " 's</w>',\n",
       " 'k',\n",
       " 'er',\n",
       " 'x',\n",
       " 'w',\n",
       " 'h',\n",
       " 'b',\n",
       " 'z',\n",
       " 'q']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokens_frequencies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('y', '</w>')\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 500\n",
    "for i in range(num_merges):\n",
    "    vocab, best = step_bpe(vocab)\n",
    "    if i % 100 == 0:\n",
    "        print('Iter: {}'.format(i))\n",
    "        print('Best pair: {}'.format(best))\n",
    "        print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['through</w>', 'ations</w>', 'tional</w>', 'before</w>', 'ation</w>', 'which</w>', 'there</w>', 'would</w>', 'their</w>', 'other</w>']\n"
     ]
    }
   ],
   "source": [
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m', 'oun', 'ta', 'in', 's</w>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenization.get(\"mountains</w>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', 'ic', 'tional</w>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenization.get(\"fictional</w>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['li', 'v', 'el', 'in', 'ess</w>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenization.get(\"liveliness</w>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a008fcbafda014ccbff718c36579044168392f5b4cc711d4367b65ab8fc3b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
